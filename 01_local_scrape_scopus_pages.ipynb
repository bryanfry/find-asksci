{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Scrape Scopus Pages\n",
    "\n",
    "This notebook includes the code to scrape the Scopus database to generate a corpus of academic journal abstracts.  It starts with a tab-delimited file listing the ~35000 journals (!) included in scopus, and uses this list to guide an extensive series of queries for abstracts.  This notebook performs the following steps:\n",
    "\n",
    "NOTE:  Scopus is a subscription service.  As of 12/2016 I have Scopus access through my graduate program at Upenn.  This code assumes that you have access and have recently logged in using your credentials.  \n",
    "\n",
    "I ran this code locally on my Macbook Pro.\n",
    "\n",
    "### Parse the Scopus-provided journal list.\n",
    "* Limits list to English-language journals\n",
    "* Removes medical / dental journals.\n",
    "* Removes multi-topic / interdisciplinary journals\n",
    "* Sorts by impact factor, so only relatively high-impact abstrats will be scraped.\n",
    "    \n",
    "### Use the cleaned-up list of Scopus journals to automate web queries. \n",
    "\n",
    "* Loops over a given range of years and loops over top N impact journals in each year. \n",
    "* Generates a Scopus URL for each journal / year.\n",
    "* Uses Selenium to click control to show 200 articles (max) and to display their abstracts.\n",
    "* Save webpage as HTML file, and update a log-file used to later index the saved HTML.\n",
    "* If > 200 articles, use Selenium to click a button to advance to next page.  Continue as needed until all abstracts for journal have been captured.\n",
    "    \n",
    "    \n",
    "NOTE:  There appears to be a memory leak in Selenium.  After running queries for several hundred journals, performance decreases notably.  All the 8 GB on my Macbook is consumed at this point.  However, killing and re-instatantiating the webdriver object appears to address this problem by freeing the leaked meemory.  This procedure is used when looping over the journals / years.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for filtering the journal list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to clean up column names in the Scopus journal file, \n",
    "# as well as names of topics.\n",
    "def clean_name (s):\n",
    "    s = s.upper()  #Make upper case\n",
    "    s = s.replace (':',' ')\n",
    "    s = s.replace ('-',' ')\n",
    "    s = s.replace (\"'\", '')\n",
    "    s = s.replace ('(', '_')\n",
    "    s = s.replace (')', '')\n",
    "    s = s.replace (',','')\n",
    "    s = '_'.join (string.split(s))\n",
    "    return s\n",
    "\n",
    "######################################################\n",
    "\n",
    "# Function to return whether journal is published in English-speaking country\n",
    "# German and Dutch journals are generally in English.  Journals from France\n",
    "# are also typically pusblished in English, but there seem to be enough exceptions\n",
    "# to justify excluding them.\n",
    "def is_english_journal (s):\n",
    "    try: return s.upper() in ['UNITED STATES','UNITED KINGDOM','AUSTRALIA','CANADA','NETHERLANDS','GERMANY']\n",
    "    except: return False\n",
    "        \n",
    "######################################################\n",
    "\n",
    "# This converts coverts the numeric topic codes to text-readable (separated by semicolons)\n",
    "def build_text_classification_codes (s, topic_dict):\n",
    "    code_list = [ i for i in s.strip().split(';')]  # List of string numerical values\n",
    "    topic_list = [topic_dict[i] for i in code_list]\n",
    "#    print topic_list    \n",
    "    return ';'.join (topic_list)\n",
    "\n",
    "######################################################\n",
    "\n",
    "# Exclude medical, dental, etc. journals.  These have structured abstracts and are challenging to parse.  \n",
    "# They 'section header' words ('Results', 'Methods', 'Conclusions', etc..) seem to bias results, but \n",
    "# these headers are vary enough between medical journals that it is not trivial to simply remove them.\n",
    "# Future work may include medical journals, but for now they are excluded from the project.\n",
    "def remove_medicine (s):\n",
    "    code_list = [ i.replace (';',' ').strip() for i in s.split() ]  # List of string numerical values\n",
    "    code_list = filter (lambda x: int (x) < 2700 or int (x) > 2799, code_list) # 2700 --> 2799 are medical topics\n",
    "    code_list = filter (lambda x: int (x) < 2900 or int (x) > 2999, code_list) # 2900 -> 2999 are nursing\n",
    "    code_list = filter (lambda x: int (x) < 3000 or int (x) > 3099, code_list) # 3000 -> 3099 are pharmocology\n",
    "    code_list = filter (lambda x: int (x) < 3400 or int (x) > 3699, code_list) # 3400 -> 3699 are veterinary / dental / health services\n",
    "    if len (code_list) < 1: return '_NONE_'  # Will be removed from the dataframe\n",
    "    else: return ';'.join (code_list)\n",
    "\n",
    "    ######################################################\n",
    "\n",
    "# Returns True if 1000 one of the topic codes, otherwise returns False.  Examples include\n",
    "# Nature, Science, PNAS.  \n",
    "# (1000 --> an 'interdisciplinary' journal)\n",
    "def is_interdisciplinary (s):\n",
    "    code_list = [ i for i in s.strip().split(';')]  # List of string numerical values\n",
    "    return '1000' in code_list\n",
    "\n",
    "######################################################\n",
    "\n",
    "# A multitopic journal is defined as having more than one topic code (this is distinct from \n",
    "# interdisciplinary, which has its own dedicated topic code.)\n",
    "def is_multitopic (s):\n",
    "    code_list = [ i for i in s.strip().split(';')]  # List of string numerical values\n",
    "    return len (code_list) > 1\n",
    "\n",
    "######################################################\n",
    "\n",
    "# Given list of topic codes, this function rounds them all down to next-lowest 100-value.\n",
    "# This makes the topics more general.\n",
    "def def_build_simple_topics (s):\n",
    "    code_list = [ int(i) for i in s.strip().split(';')]  # List of string numerical values\n",
    "    code_list = [i - i%100 for i in code_list]\n",
    "    code_list = list(set(code_list)) # Uniquify\n",
    "    code_list = [str(i) for i in code_list]\n",
    "    return ';'.join (code_list)\n",
    "\n",
    "######################################################\n",
    "\n",
    "# Function to generate 'electronic ISSN or E_ISSN'\n",
    "# The scopus query works most reliably using an electronic ISSN rather\n",
    "# than a text journal name.  Some journals have an E_ISSN given in the \n",
    "# scopus journal list.  For those that do not, we use this function to \n",
    "# generate the E_ISSN from a given print ISSN.\n",
    "def generate_e_issn (print_issn, e_issn):\n",
    "    if e_issn == 'NONE':\n",
    "        try:\n",
    "            e_issn = '0' * (8-len(print_issn)) + print_issn\n",
    "            e_issn = e_issn[0:4] + '-' + e_issn[4:]\n",
    "        except: e_issn = 'NONE'\n",
    "    return e_issn\n",
    "\n",
    "#print generate_e_issn ('7434618','14773848')\n",
    "#print generate_e_issn ('255858','NONE')\n",
    "#print generate_e_issn (np.nan, 'NONE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate Scopus query URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.scopus.com/results/results.uri?sort=plf-f&src=s&sid=DD71260EDC8DA0916BED214D5039CFAC.wsnAw8kcdt7IPYLO0V48gA%3a130&sot=a&sdt=a&sl=71&s=ISSN%2815205126%29+AND+PUBYEAR+%3d+2015+AND+DOCTYPE%28ar%29+AND+LANGUAGE%28english%29&origin=searchadvanced&editSaveSearch=&txGid=DD71260EDC8DA0916BED214D5039CFAC.wsnAw8kcdt7IPYLO0V48gA%3a13'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_url (e_issn, yr = 2015, language = 'english'):\n",
    "    pass\n",
    "    # The sample URL was generated from the Scopus advanced feature.  ISSN is the electronic\n",
    "    # journal code, in this case for JACS.  Language is english, and year is 2015, and\n",
    "    # document type is articles (i.e. not conference proceedings, etc.)\n",
    "    \n",
    "    r=    'https://www.scopus.com/results/results.uri?sort=plf-f&src=s'\n",
    "    r=r + '&sid=DD71260EDC8DA0916BED214D5039CFAC.wsnAw8kcdt7IPYLO0V48gA%3a130&sot=a&sdt=a&sl=71'\n",
    "    r=r + '&s=ISSN%28'\n",
    "    r=r +  str (e_issn)\n",
    "    r=r + '%29+AND+PUBYEAR+%3d+'\n",
    "    r=r +  str (yr)\n",
    "    r=r +  '+AND+DOCTYPE%28ar%29+AND+LANGUAGE%28'\n",
    "    r=r +  language\n",
    "    r=r +  '%29&origin=searchadvanced&editSaveSearch='\n",
    "    r=r +  '&txGid=DD71260EDC8DA0916BED214D5039CFAC.wsnAw8kcdt7IPYLO0V48gA%3a13'\n",
    "    \n",
    "    return r\n",
    "\n",
    "#build_url (15205126)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous helper functions for Scopus queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace commas in dataframe with underscores\n",
    "\n",
    "def replace_commas(s):\n",
    "    try: return s.replace(',','_')\n",
    "    except: return s\n",
    "    \n",
    "######################################################\n",
    "\n",
    "# Compute number of queries needed for given journal / year combination.\n",
    "# Scopus returns only 200 articles per query, so esssentially this \n",
    "# is a rounded-up mod div by 200.\n",
    "\n",
    "def calc_n_queries (n):   # n = number of articles in journal for year\n",
    "    if n < 1: return 0\n",
    "    else: return min (1 + ((n-1) // 200), 10)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "# Build filename for an HTML page.\n",
    "\n",
    "# e_issn is journal tag, q is query number (1..10)\n",
    "def build_html_filename (j, e_issn, q, yr):\n",
    "    return str (yr) + '_' + str (j).zfill (5) + '_' + e_issn + '_' + str (q).zfill(2)+'.html'\n",
    "    \n",
    "\n",
    "#####################################################\n",
    "\n",
    "# Set the Scopus website to show 200 articles (max #), and to display abstracts.\n",
    "\n",
    "def config_abstract_view (driver):\n",
    "    \n",
    "    #First set to view 200 docs:\n",
    "    n_res_dropdown = driver.find_element_by_id('resultsPerPage-button')\n",
    "    n_res_dropdown.send_keys('200\\n')\n",
    "    \n",
    "    # Now make the abstracts visible\n",
    "    try:\n",
    "        abstract_toggle = driver.find_element_by_link_text ('Show all abstracts')\n",
    "        abstract_toggle.click()\n",
    "    except:    \n",
    "        pass #abstracts were already visible, no problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to intialize and write to the scopus query log file.  \n",
    "\n",
    "This log file contains one line for each page returned by the scopus site.  It is used to track success / failures in the query, and has information to guide the (later) process of extracting HTML from the saved pages and saving the results to Mongo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This log is a tab-delimited file, should be readable as a dataframe.\n",
    "# f_log      = file handle to the output log\n",
    "# fn_html    = file name for html file of just-written page\n",
    "# e_issn     = e_issn of current journal\n",
    "# n_article  = total # of articles for the journal\n",
    "# b_err      = Boolean, True is the scopus query failed\n",
    "# df_jist    = the whole dataframe for journals.  The row corresponding to the E_ISSN will be\n",
    "#              copied to the log\n",
    "\n",
    "def write_to_log_file (f_log, yr, fn_html, e_issn, n_articles, b_err, df_jlist):\n",
    "    t = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\") # current time\n",
    "    df_row = df_jlist[df_jlist.E_ISSN == e_issn]   # extract row from dataframe\n",
    "    df_row = df_row.to_csv(header=False, index=False)\n",
    "    s = ','.join ([t, str (yr), fn_html, str(n_articles), str(b_err)])\n",
    "    s =  s + ',' + df_row \n",
    "    f_log.write (s)\n",
    "    f_log.flush()\n",
    "    return\n",
    "    \n",
    "######################################################\n",
    "    \n",
    "# Function to open a new log file and write its header line  \n",
    "def init_log_file (fp_log_out, df_jlist):\n",
    "    if os.path.exists (fp_log_out):\n",
    "        f_log = open (fp_log_out, 'a')\n",
    "    else:\n",
    "        f_log = open (fp_log_out, 'w')\n",
    "        s = 'TIMESTAMP,YEAR,HTML_FILE,N_ARTICLES,ERR,'\n",
    "        s = s + ','.join (df_jlist.columns) + '\\n'\n",
    "        f_log.write (s)\n",
    "    return f_log\n",
    "\n",
    "######################################################\n",
    "\n",
    "# Function to log a chromedriver restart\n",
    "def write_to_log_file_chromedriver_restart (f_log):\n",
    "    t = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\") # current time\n",
    "    s = t + ',' + 'CHROMEDRIVER KILL / RESTART\\n'\n",
    "    f_log.write (s)\n",
    "    f_log.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make a scopus query (up to 200 docs), save the HTML, and update the log file\n",
    "\n",
    "The text following is a sample Scopus query, for the Journal of the American Chemical Society.  It should return a query result if you are properly configured for Scopus access through your institution.\n",
    "\n",
    "**https://www.scopus.com/results/results.uri?sort=plff&src=s&sid=DD71260EDC8DA0916BED214D5039CFAC.wsnAw8kcdt7IPYLO0V48gA%3a130&sot=a&sdt=a&sl=71&s=ISSN%2815205126%29+AND+PUBYEAR+%3d+2015+AND+DOCTYPE%28ar%29+AND+LANGUAGE%28english%29&origin=searchadvanced&editSaveSearch=&txGid=DD71260EDC8DA0916BED214D5039CFAC.wsnAw8kcdt7IPYLO0V48gA%3a13**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f_log = file handle to the log file\n",
    "# j = index for current journal \n",
    "# e_issn = e_issn for current journal\n",
    "# n_docs = total number of docs for currnet journal\n",
    "# q = current query number for current journal (1..10)\n",
    "# n_query = total number of queries needed for current journal\n",
    "# df_jlist = big dataframe of journal info\n",
    "# driver = the selenium broweser driver object\n",
    "# err:  Will be TRUE if an error occurred prior to calling the function.  If\n",
    "#       err == True, no query / html save will be doen here.  However, the log is still \n",
    "#       written to make record of the error.\n",
    "\n",
    "def save_scopus_query (dir_out_html, f_log, j, e_issn, n_docs, q, yr, n_query, df_jlist, driver, err):\n",
    "        \n",
    "        if err == False:      \n",
    "            try:\n",
    "                print 'Running query ' + str(q) + ' of ' + str (n_query)\n",
    "                if q == 1:\n",
    "                    config_abstract_view (driver)   # Make abstracts visible, set to 200 / page\n",
    "                src = driver.page_source \n",
    "                fn_html = build_html_filename (j, e_issn, q, yr)\n",
    "                time.sleep(sleep_time_query)\n",
    "                fp_html = os.path.join (dir_out_html, fn_html)\n",
    "                print fp_html\n",
    "                with open (fp_html, 'w') as f:\n",
    "                    f.write (src.encode ('UTF-8'))    \n",
    "            except: err = True\n",
    "                \n",
    "        if err == True:  fn_html = 'ERR_NO_VALID_HTML_DUMP'\n",
    "                \n",
    "        write_to_log_file (f_log, yr, fn_html, e_issn, n_docs, b_err = err, df_jlist = df_jlist)    \n",
    "        \n",
    "        return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update restart counter, and kill / restart chromedriver if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_chromedriver_reset (driver, q_since_reset, browser_restart_interval, f_log, delay = 3):\n",
    "    q_since_reset = (q_since_reset + 1) % browser_restart_interval\n",
    "    \n",
    "    if q_since_reset == 0:\n",
    "        print 'KILLING CHROMEDRIVER'\n",
    "        driver.close()\n",
    "        time.sleep (delay)\n",
    "        print 'RESTARTING CHROMEDRIVER'\n",
    "        driver = webdriver.Chrome (chromedriver)\n",
    "        time.sleep(delay)\n",
    "        write_to_log_file_chromedriver_restart (f_log)\n",
    "    \n",
    "    return driver, q_since_reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN -- Filter Journal List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF JOURNALS REMOVED FOR MEDICAL / VET / NURSING TOPICS = 2982\n",
      "TOTAL # Journal = 11413\n",
      "TOTAL # of Multidisciplinary Journals is = 34\n",
      "TOTAL # of NOT multi-topic Journals is = 6286\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Define filepaths\n",
    "fp_jlist_in = '/Users/bryanfry/projects/proj_asksci/files_in/scopus_journal_list_tab_delimited.txt'\n",
    "fp_topics_in = '/Users/bryanfry/projects/proj_asksci/files_in/scopus_topic_codes.csv'\n",
    "fp_jlist_out = '/Users/bryanfry/projects/proj_asksci/files_out/filtered_journal_list.txt'\n",
    "\n",
    "# Read the csv files\n",
    "df_j = pd.read_csv (fp_jlist_in, sep = '\\t')\n",
    "df_t = pd.read_csv (fp_topics_in)\n",
    "\n",
    "# Clean up the column names in the journal and topics list dataframes\n",
    "df_j.columns = [clean_name(i) for i in df_j.columns]\n",
    "df_t.columns = [clean_name(i) for i in df_t.columns]\n",
    "\n",
    "# Build missing E_ISSN values (electronic journal ID codes)\n",
    "df_j.E_ISSN =   df_j.E_ISSN.fillna('NONE') # Fill missing E_ISSN with 'NONE'\n",
    "\n",
    "# String algebra to generate correct E_ISSN for journals where it is absent\n",
    "df_j.E_ISSN = [generate_e_issn (i,j) for i,j in zip (df_j.PRINT_ISSN, df_j.E_ISSN)]\n",
    "\n",
    "# Clean up the topic names\n",
    "df_t.DESCRIPTION = df_t.DESCRIPTION.apply (clean_name)\n",
    "\n",
    "# Basic filtering of the journal list\n",
    "df_j = df_j [df_j.SOURCE_TYPE.apply(string.upper) == 'JOURNAL'] # Limit to journals (not books, etc)\n",
    "df_j = df_j [df_j.ACTIVE_OR_INACTIVE.apply(string.upper) == 'ACTIVE'] #Limit to journals active in Scotus\n",
    "df_j = df_j [df_j.PUBLISHERS_COUNTRY.apply (is_english_journal) == True] # Limit to likely english language journals\n",
    "\n",
    "# Sort on 2015_SNIP, descending (this is a topic-normalized impact factor)\n",
    "df_j = df_j.sort_values (['2015_SNIP'], ascending=False)\n",
    "\n",
    "# Remove medicine / vet / nursing / veterinary topics, then remove journals for which no topics remain.\n",
    "len_org = len (df_j)\n",
    "df_j.ALL_CLASSIFICATION_CODES = df_j.ALL_CLASSIFICATION_CODES.apply (remove_medicine)\n",
    "df_j= df_j[df_j.ALL_CLASSIFICATION_CODES != '_NONE_']\n",
    "print 'NUMBER OF JOURNALS REMOVED FOR MEDICAL / VET / NURSING TOPICS = ' + str (len_org - len (df_j))\n",
    "\n",
    "# Build column for interdisciplinary journal - True or False\n",
    "df_j['IS_INTERDISCIPLINARY'] = df_j.ALL_CLASSIFICATION_CODES.apply (is_interdisciplinary)\n",
    "\n",
    "# Build columnd for 'simple classification codes' -- all topic codes rounded to next-lowest mult of 100.\n",
    "df_j['ALL_SIMPLE_CODES'] = df_j.ALL_CLASSIFICATION_CODES.apply (def_build_simple_topics)\n",
    "\n",
    "# Make a dictionary with the topic codes as keys and subjects as values\n",
    "topic_dict = {str(df_t.CODE.iloc[i]):df_t.DESCRIPTION.iloc[i] for i in range (len(df_t))}\n",
    "\n",
    "# Add a field indicating multi-topic journal (more than one 100-level topic)\n",
    "df_j['IS_MULTITOPIC'] = df_j.ALL_SIMPLE_CODES.apply (is_multitopic)\n",
    "\n",
    "# Finally, add new columns that includes text topics separated by semicolons.\n",
    "df_j['ALL_CLASSIFICATION_TOPICS'] = [build_text_classification_codes(i, topic_dict) for i in df_j.ALL_CLASSIFICATION_CODES]\n",
    "df_j['ALL_SIMPLE_TOPICS'] = [build_text_classification_codes(i, topic_dict) for i in df_j.ALL_SIMPLE_CODES]\n",
    "\n",
    "\n",
    "df_j.to_csv (fp_jlist_out, sep='\\t', index = False)\n",
    "\n",
    "\n",
    "print 'TOTAL # Journal = ' + str (len (df_j))\n",
    "print 'TOTAL # of Multidisciplinary Journals is = ' + str (len (df_j[df_j.IS_INTERDISCIPLINARY == True]))\n",
    "print 'TOTAL # of NOT multi-topic Journals is = ' + str (len (df_j[df_j.IS_MULTITOPIC == False]))\n",
    "print 'DONE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Main -- Query Scopus and save webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Point to the Selenium Chromedriver\n",
    "chromedriver = '/Users/bryanfry/chromedriver'\n",
    "os.environ ['webdriver.chrome.driver'] = chromedriver\n",
    "\n",
    "# Input file with cleaned journal list\n",
    "fp_jlist_in = '/Users/bryanfry/projects/proj_asksci/files_out/filtered_journal_list.txt' \n",
    "\n",
    "# All saved webpages, and the query log file, will be saved to the following directory.\n",
    "# Directory will be created if it does not already exist.\n",
    "dir_out = '/Users/bryanfry/projects/proj_asksci/files_out/scopus_query_out'\n",
    "\n",
    "#Log file, updated with each saved scopus HTML page\n",
    "fp_log_out = os.path.join (dir_out, '_SCOPUS_QRY_LOG.csv')\n",
    "\n",
    "sleep_time_init = 30  #Intial time to sleep before first query (sec)\n",
    "sleep_time_query = 1  #Time to sleep after each query (sec)\n",
    "#end_journal = 2500\n",
    "end_journal = 10  # Number of journals to run.  The journals will be run in order of decreasing 20515_SNIP\n",
    "start_journal = 0  #Initial journal in the list, used to restart mid-run. 0 --> first journal.\n",
    "yr_list = [2015, 2013, 2011, 2009, 2007]  # Updated to support mulitple years\n",
    "browser_restart_interval = 100 # Every N queries, the chromedriver will be killed and restarted.... this\n",
    "                              # aims to address the memory leak issue seen in long runs\n",
    "\n",
    "# Create Scopus query output directory, if it does not exist:\n",
    "if not os.path.exists (dir_out): os.makedirs(dir_out)\n",
    "    \n",
    "n_cum_docs = 0 # Cumulative count of documents\n",
    "q = 0  # Total query count\n",
    "q_since_reset = 0  # Queries since last chromedriver kill/restart\n",
    "\n",
    "df_jlist = pd.read_csv (fp_jlist_in, sep = '\\t')  # Read *.TXT with journal info (tab-delimited)\n",
    "df_jlist = df_jlist [df_jlist.IS_MULTITOPIC == False]  # Eliminate Multi-topic Journals\n",
    "\n",
    "f_log = init_log_file (fp_log_out, df_jlist)  # Initialize log file (one line per scopus query)\n",
    "\n",
    "df_jlist.E_ISSN =   df_jlist.E_ISSN.fillna('NONE') # Fill missing E_ISSN with 'NONE'\n",
    "\n",
    "# String algebra to generate correct E_ISSN for journals where it is absent\n",
    "df_jlist.E_ISSN = [generate_e_issn (i,j) for i,j in zip (df_jlist.PRINT_ISSN, df_jlist.E_ISSN)]\n",
    "\n",
    "# If both electronic and print ISSN values were not given in the original file, the E_ISSN field will now contain\n",
    "# 'NONE'.  Remove journals where this is the case.\n",
    "df_jlist = df_jlist [df_jlist.E_ISSN != 'NONE']\n",
    "\n",
    "# replace any commas in the journal dataframe with underscores.  There are some\n",
    "# commas in various fields (publisher names, etc.) and they may confuse the comma-delimited\n",
    "# file format\n",
    "for c in df_jlist.columns:\n",
    "    df_jlist[c] = df_jlist[c].apply (replace_commas)\n",
    "\n",
    "    \n",
    "df_jlist = df_jlist.sort_values (by=['2015_SNIP'], ascending=False) #Sort by impact factor\n",
    "\n",
    "driver = webdriver.Chrome (chromedriver)\n",
    "time.sleep (sleep_time_init)\n",
    "end_journal = min (end_journal, len (df_jlist)-1) # Do not allow end_journal to exceed # of valid journals in dataset.\n",
    "\n",
    "\n",
    "# LOOP ON YEARS\n",
    "for yr in yr_list:\n",
    "    # Now we will loop on the top N journals, making a scopus query for each one.\n",
    "    for idx, e_issn in enumerate (df_jlist.E_ISSN[start_journal: (end_journal+1)]):\n",
    "        j = idx + start_journal  # journal index, offset by the starting point\n",
    "        err = False   # New Journal, reset the error flag\n",
    "        print 'E_ISSN = ' + e_issn + '                  J# = ' + str (j)\n",
    "        print 'JOUNRAL = ' + df_jlist.JOURNAL_TITLE[df_jlist.E_ISSN == e_issn].tolist()[0]\n",
    "        print 'YEAR = ' + str(yr)\n",
    "        url = build_url (e_issn, yr = yr, language = 'english')  # construct URL for the page\n",
    "        try:\n",
    "            driver.get (url)  # Load the page\n",
    "            # Read in the total number of articles for the journal / year\n",
    "            count_display = driver.find_element_by_class_name ('resultsCount')\n",
    "            n_docs = int (count_display.text.replace (',',''))\n",
    "            n_cum_docs = n_cum_docs + n_docs\n",
    "            print 'CUMULATIVE DOC COUNT (ACROSS ALL YEARS) = ' + str (n_cum_docs)\n",
    "        except: err = True\n",
    "\n",
    "        # Compute the number of scopus pages needed to get the abstracts (max of 2000 abstracts)\n",
    "        # Loop to execute the queries\n",
    "        n_query = calc_n_queries (n_docs)\n",
    "        if n_query > 0:\n",
    "            q = 1\n",
    "            save_scopus_query (dir_out, f_log, j, e_issn, n_docs, q, yr, n_query, df_jlist, driver, err) #save html, update log\n",
    "            q = q + 1\n",
    "            driver, q_since_reset = process_chromedriver_reset (driver, q_since_reset, \\\n",
    "                                                                browser_restart_interval, f_log, delay = 3)\n",
    "            # If necessary, click the \"next page\" button and make next query\n",
    "            while q <= n_query and err == False:\n",
    "\n",
    "                # Find and click the \"Next page\" button\n",
    "                try:\n",
    "                    next_page = driver.find_element_by_class_name('nextPage')\n",
    "                    next_page.click()\n",
    "                except: err = True\n",
    "\n",
    "                save_scopus_query (dir_out, f_log, j, e_issn, n_docs, q, yr, n_query, df_jlist, driver, err) #save html, update log\n",
    "                q = q + 1\n",
    "                driver, q_since_reset = process_chromedriver_reset (driver, q_since_reset, \\\n",
    "                                                                    browser_restart_interval, f_log, delay = 3)\n",
    "                \n",
    "f_log.close()    \n",
    "driver.close()\n",
    "print '\\n#### DONE #####'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
