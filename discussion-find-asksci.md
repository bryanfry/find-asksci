# Discussion of *find-asksci*

## Introduction

**[find-asksci](http://34.196.238.24)** is a web application that aims to locate the redditor in the subreddit /r/askscience who is best-qualified to answer a user's technical question.  I developed this application as my final project at the Metis Data Science Bootcamp in NYC.  This project involved web scraping, natural language processing (NLP), dimensionality reduction, machine learning, visualization, and Flask app development.

As of 1/11/17, the app is hosted on an Amazon Web Services EC2 instance.  Please click [here](http://34.196.238.24) to give it a try!   

_(**HINT:** asking more detailed questions including technical / scientific terminology will usually give the best results.)_

My [github repository](http://github.com/bryanfry/find-asksci) for the project includes the development and production code for the find-asksci app. **Five iPython notebooks** (numbered 01 - 05) illustrate the scraping, data manipulation, and machine learning yielding the models behind the web application.  The first two notebooks were run locally on my MacBook.  I ran notebooks 03 - 05 on a compute-optimized EC2 instance with 30 GB RAM, as they were too memory-intensive to execute successfully  on my local machine.  The web application itself is a Python Flask app (hosted with the Apache webserver) that relies on large pickled data structures produced by the initial iPython notebooks.  The HTML for the webpage is also included.  

The remainder of this document discusses the goals, design, and implementation of the project.  Please take a look at the code if you are interested!

## Dependencies (development and production code)

* pandas
* numpy
* Selenium
* MongoDB + pymongo
* sklearn
* matplotlib + seaborn
* Flask  
* Online access to Scopus database of academic journals

## Overview

Reddit provides a vast range of forums for discussion and content aggregation.  The site is divided into thousand of individual discussion forums ("fora?") known as subreddits.  Many of these, such as /r/funny and /r/pics, primarily offer light entertainment.  However, other subreddits are sources of more serious discussion.  /r/askscience is a popular subreddit where users ask technical or scientific questions; often redditors with strong professional or academic backgrounds will take the time to address these questions.  Given a user question, the **find-asksci** app locates the /r/askscience author whose total comment corpus best aligns with the question, based on **cosine similarity** following **TFIDF vectorization** and **Singular Value Decomposition (SVD)** transformation.  

I trained the NLP models on over 800,000 journal abstracts from 2007 - 2015.  To obtain the abstracts I employed Selenium to scrape the website of Scopus, the largest database of academic journals.  Training directly on the reddit r/askscience comment set may have offered a more straightforward approach.  However, in addition to solid technical discussion, the reddit corpus also includes a large amount of 'other' content including additional questions and general commentary and banter.  Journal abstracts, in contrast, present a highly distilled corpus of technical expertise.  Training on abstracts, rather than reddit itself, should thus improve the 'signal-to-noise' in the transform and the subsequent content matching.

I built this program mainly to illustrate some interesting techniques in natural language processing, and it also offers a practical alternative to reddit's (somewhat notorious) native search functionality.  More broadly, expertise identification is an important real-world problem.  Successfully automating such tasks is important for academics, recruiters, technical marketers and salespeople, and managers at large and diversified organizations.  

## Step #1 -- Scrape Scopus (run locally)

The notebook **01\_local\_scrape\_scopus\_pages.ipynb** uses Selenium to query the Scopus database for all abstracts from the 2000 "top" journals for 2015, 2013, 2011, 2009, and 2007.  The full HTML for each query is saved locally, and the abstract text is parsed out later.  The full query ran overnight on my Macbook.  

### Journal selection
Scopus references over 35,000 journals and provides information on their broad and specific topics, place of publication, etc.  I arranged the journal list in descending order of 2015 impact factor (a representation of journal 'importance' or 'quality'), and removed some journals based on topic and language (see code).  

## Step #2 -- Extract Abstracts (run locally)

**02\_local\_html\_to\_mongo.ipynb** reads through the several thousand HTML files generated by the previous notebook to extract the abstracts.  The abstracts, along with some other information including citations and authors, were saved locally in a MongoDB collection.  I employed BeautifulSoup to parse the HTML. 

I used the Mongo Dump utility to save the database to a single file (~ 1 GB), which was then transferred to an AWS instance offering more RAM for subsequent processing.

## Step #3 -- Parse /r/askscience archive (run on AWS)

Before executing the third iPython notebook, I used a tool developed by Github user 'peoplma' to obtain JSON representations of all the askscience comments from 2012-2016.  This tool, built against PRAW (the standard Python reddit API) is available in github [here] (https://github.com/peoplma/subredditarchive). 

**03\_aws\_process\_reddit\_archive.ipynb** reads the JSON files and builds a python dictionary mapping each unique author in \r\askscience to a list of all of his/her comments over the 5-year period.  The reddit comment set for each post is an arbitrarily deep tree, and this structure is reflected in the JSON.  A recursive Python function parses the tree, ultimately called on all child comments of the original post.  Interestingly, the distribution of comments across redditors is very uneven.  Of the 181521 authors in the archive, nearly 50% only wrote a single comment!  The following histogram shows this distribution (please note log scale on Y-axis).

![Redditor Comment Histogram](http://34.196.238.24/static/images/redditor_histogram.png)

A cursory examination of the corpus showed (unsurprisingly) that those redditors with more than a just a handful of comments typically seemed to possess greater technical expertise.  To improve to average quality of the comments, I chose to limit the redditor set to only those redditors that had provided at least 40 comments, equivalent to removing the leftmost bar in the histogram.  This reduced the number of authors from 181521 to 2753.  However, of the original 899699 comments, 350160 remained after this change due the extreme imbalance in the distribution of comments.

## Step #4 -- Build NLP Models (run on AWS)

### NLP Model Overview

**04\_aws\_build\_nlp\_models.ipynb** reads the journal abstracts from the Mongo database and to train Natural Language Processing models.  First, the abstracts are vectorized using term frequency / inverse document frequency (TFIDF) scheme.  Next, the vectorized abstracts are reduced from ~45000 to 200 dimensions using truncated Singular Value Decomposition (SVD).  This vectorization and dimensionality reduction is used to assign cosine similarity in the redditor matching application.  

### Validating Dimensionality Reduction

Scopus provides topic labels for their journals, covering topics like PHYCHOLOGY, CHEMISTRY, and EARTH_AND_PLANETARY_SCIENCES.  Machine learning classifiers can easily be trained on these labels, aiming to match abstracts to topics.  If topic classification accuracy does not suffer greatly from dimensionality reduction, this provides a good sanity check for moving forward with the SVD scheme. **Please see the iPython notebook for details and results**

I test / train splits on the abstracts and topic labels using Support Vector (SVC) and Random Forest Classifiers, both with and without SVD dimensionality reduction.  The overall accuracy results are as follows:

* SVC: 83%
* SVC, 200-dim Truncated SVD: 75%
* Random Forest: 71%
* Random Forest, 200-dim Truncated SVD: 73%

SVC accuracy does fall with dimensionality reduction, while surprisingly the accuracy of the Random Forest Classifier actually _increases_ slightly!  In any case, it does not seem reasonable to expect accuracy approaching 100% in any case given topic overlap and the interdisciplinary nature of many journal articles.  A confusion matrix is shown below.  Higher rates of error can be seen in topics that intuitively would be expected to show some overlap (Chemistry and Chemical Engineering, for example). 

![Sample Confusion Matrix](http://34.196.238.24/static/images/conf_matrix.png)

Interesting, I tried using Non-Negative Matrix Factorizaiton (NMF) in lieu of SVD to reduce dimensions, and the performance was quite poor.  Classification accuracy was consistently less than 50% with NMF.

### Transforming the Reddit Comments
The pickled /r/askscience archive (from previous iPython notebook) was reloaded.  For each author, I concatenated all comments into a single long document and then applied the same TFIDF and SVD transformations used on the abstract corpus.  Thus, each redditor was mapped to a 200-element vector describing their expertise.  The redditor - to - "expertise vector" map was saved as a dataframe for future use.  This scheme is scheme is simple and generally appears to work well for matching expertise in the final flask app.  However, it implicitly assumes that each redditor limits their comments to a fairly narrow area of knowledge.  Perhaps a more precise matching scheme could compare cosine similarity to all individual comments, and identify a best author match based on the number of comments whose similarity to the question exceeds some threshold.  However, this would be considerably more computationally expensive (cosine similarity to 350000 comment scores versus 2753 author scores).

## Step #5 -- Prototype Redditor Expertise Matching (run on AWS)
The last iPython notebook, **05\_test\_redditor\_identification.ipynb**, prototypes the function used in the Flask app to identify /r/askscience experts.  After unpickling the redditor archives and trained NLP models and ingesting a user question string, it performs the following steps:

* Apply TFIDF and SVD transformations to question
* Use SVC topic model to return most likely 'broad topic' for the question ('ASTRONOMY AND PHYSICS', etc.)
* Compute cosine similarity between question and SVD-transformed _total corpus_ for each redditor, and identify 'most similar' author
* Apply TFIDF and SVD to each _individual comment_ of that author
* Compute cosine similarity between question and each comment of 'most similar author', and return most similar comment

Several sample questions, and their results, are included in the notebook.

Finally, I looked for visible clusters in the topic coordinates of the reddit comments.  I applied a topic label for each reddit comment using the support vector classifier.  The following figure is a 2-dimenstional t-Stochastic Embedded Neighbor (TSNE) representation of the 200-dimensional topic vector for 6000 comments in 5 topic areas.  There is clear structure related to toepic in these comments, and it is interesting that some topics seem to form two or more distinct, well-contained clusters.

![Reddit Comment TSNE](http://34.196.238.24/static/images/tsne.png)

## The [find-asksci] (http::/34.196.238.24) app (run on AWS)
The app is hosted on a persistent AWS EC2 instance. It uses jQuery to call the Flask program encapsulating the expertise-matching function.  The pickled NLP models consume ~ 2 GB RAM, and take several minutes to load.   However, the real-time performance of the app is speedy because it needs to perform NLP only on small strings in real-time (user question and 40 - 2000 reddit comments).  Likewise, calculating cosine similarity between the user question and ~ 2000 vectors is not computationally demanding.

The production code resides in **flaskapp.py** and **interactive.html**.

## Final Thoughts 

This NLP project is somewhat unusual in that trains an NLP model on one corpus (scientific journal abstracts) and subsequently uses the model to transform and classify content that potentially may be quite different (reddit comments and user questions).  It would be interesting to train the models on reddit directly and compare the results.

Independent of the reddit application addressed here, the Scopus journal abstract corpus is a rich dataset that could offer a starting point for a number of interesting projects.

* Based on language content, have single-topic journals become more or less distinct over time? (HYPOTHESIS: they have become less distinct, reflecting a rise in interdisciplinary work and 'blurring' between academic discipline).
* Can certain terms (or dimensions following SVD or other dimensionality reduction) be associated with well-cited articles?  
	* Can hot topics in science be identified in this way, and can their 'level of hotness' be extrapolated to future prediction?
	*  How does non-technical language in the abstract correlate with citations?  Are the conclusions of impactful papers presented more or less forcefully, for instance?






