{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 02 - Parse HTML and insert into MongoDB\n",
    "\n",
    "The previous notebook used Selenium to save  Scopus queries (each with up to 200 abstracts) as HTML files.  This notebook loads the previously-saved HTML files and extracts the abstract text (along with some other info like the journal, year, authors, etc).  The extracted information is then saved to a Mongo database.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import string\n",
    "import numpy as np\n",
    "import sys\n",
    "import unicodedata\n",
    "import nltk\n",
    "import nltk.data\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to convert generic string to unicode\n",
    "# http://stackoverflow.com/questions/22474701/mongodb-insertion-shows-strings-in-documents-must-be-valid-utf-8\n",
    "\n",
    "def to_unicode (d):\n",
    "    #return unicodedata.normalize('NFKD', d).encode('ascii','ignore')\n",
    "    return unicode (d, errors = 'replace')\n",
    "\n",
    "##########################################\n",
    "\n",
    "# Function to parse the filename of HTML file and return its separate components\n",
    "# Splits the filename on underscores and returns the values:\n",
    "# Year\n",
    "# Journal index\n",
    "# Electronic ISSN\n",
    "# File index for given journal (can be non-zero if the journal / year had more than 200 articles)\n",
    "\n",
    "def parse_html_filepath (fp_html):\n",
    "    s = os.path.splitext (os.path.split (fp_html) [1])[0] # Isolate filename (end of path)\n",
    "                                                          # and remove extension\n",
    "    s_list = s.split('_')  # separate filename on underscores\n",
    "    return (int (s_list[0]), int(s_list[1]), s_list[2], int (s_list[3])-1)\n",
    "\n",
    "###############################################\n",
    "\n",
    "# Function to extract a 'minipath' from a path string -- this just returns the last n path\n",
    "# elements.\n",
    "def minipath (fp, n_elements = 2):\n",
    "    path_list = fp.split('/')\n",
    "    return ('/'.join (path_list [-1*n_elements:]))\n",
    "\n",
    "###############################################\n",
    "\n",
    "# Function to remove 'Kill / Restart Chromedriver' rows from logfile dataframe\n",
    "\n",
    "def is_chromedriver_remark (s):\n",
    "    return 'CHROMEDRIVER' in s\n",
    "    \n",
    "################################################\n",
    "\n",
    "# Function to remove any non-printable chatacter from string\n",
    "\n",
    "def remove_unprintable (s):\n",
    "    try: s = filter (lambda x: x in string.printable, s)\n",
    "    except: pass\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to open an HTML file and return soup and list of < li > objects, each of which contains information for one article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open an html doc and:\n",
    "#   - convert to beautifulSoup object\n",
    "#   - return li_list, which is a list of html <li> objects, each of which\n",
    "#     contains the information for a given article.\n",
    "#   - The soup object and the <li> list used for subsequent HTML parsing.\n",
    "\n",
    "def scopus_query_html_to_soup (fp_html):\n",
    "    with open (fp_html) as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup (html, 'lxml')\n",
    "    li_list_key = soup.find(name = None, attrs = {'id':'srchResultsList'})\n",
    "    li_list = li_list_key.find_all ('li')\n",
    "    return soup, li_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to parse the soup for a single article.  Requires soup object, as well as list of < li > objects, and the index ('doc_num') for desired article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a doc number, function to return the abstract, title, etc. from the HTML\n",
    "# Doc num passed in should be 1-indexed.\n",
    "# The field for the abstract is also 1-indexed.  However, the\n",
    "# field for the list item owning the other results is zero-indexed is zero-indexed.\n",
    "# li_list is a list of <li> elements, found previously.\n",
    "\n",
    "# If no abstract was returned by Scopus, abstract field is NONE.\n",
    "\n",
    "def get_doc_info (soup, doc_num, li_list):\n",
    "    \n",
    "    # Put this HTML processing block in a try.\n",
    "    # Known modes of failure....\n",
    "    #    1) At least once the <li> had a bunch of <a> tags BEFORE the one with the\n",
    "    #    article title and link.  The find returned this tag, and threw an error because\n",
    "    #    the text (title) and href were not present and could not be extracted.\n",
    "    try:\n",
    "        ab = soup.find (name=None, attrs = {'id':'previewAbstract' + str(doc_num + 1)})\n",
    "        ab_text = ab.text.strip()\n",
    "        if ab_text[0:24] == '[No abstract available]':\n",
    "            ab_text = 'NONE'\n",
    "        else:\n",
    "            ab_text = ab.text.replace ('\\n','  ').strip()  # Remove line breaks in the abstract\n",
    "\n",
    "\n",
    "        li_key = li_list [doc_num] # Get the correct item in the results key list (0-indexed)\n",
    "\n",
    "        ti_key = li_key.find (name = None, attrs = {'class':'docTitle'})  # Get the document title obj\n",
    "        a_key = ti_key.find ('a')  # An anchor key in the ti_key has the url and the title text\n",
    "        ti_text = a_key.text\n",
    "        doc_url = a_key['href']\n",
    "\n",
    "        author_col = li_key.find (name = None, attrs = {'class':'dataCol3'})  # This column has author info\n",
    "        a_list = author_col.find_all ('a')  # There is an anchor tag here for each (displayed) author\n",
    "        author_list_names = [i.text for i in a_list] #Extract author names from the anchor tags\n",
    "        author_list_urls = [i['href'] for i in a_list] #Extract link urls from the anchor tags\n",
    "\n",
    "        citation_col = li_key.find (name = None, attrs = {'class':'dataCol6'}) # Column with 'cited by'\n",
    "    \n",
    "    except:\n",
    "        ti_text, ab_text, doc_url, n_citations, url_citations, author_list_names, author_list_urls = \\\n",
    "        '', '', '', 0, '', [], []\n",
    "        return (ti_text, ab_text, doc_url, n_citations, url_citations, \\\n",
    "         author_list_names, author_list_urls)\n",
    "    \n",
    "    # Use a try / except here because we will have an excepton in the 0-citation case.\n",
    "    try:\n",
    "        cite_tag = citation_col.find ('a') #An anchor tag in the column has the info\n",
    "        n_citations = int (cite_tag.text.split()[0])\n",
    "        url_citations = cite_tag['href']\n",
    "    except:  #No citations\n",
    "        n_citations = 0\n",
    "        url_citations = ''\n",
    "    \n",
    "    # return all the strings encoded as UTF-8, so they can be written to file\n",
    "    # without any grief\n",
    "    author_list_names = [i.encode ('UTF-8') for i in author_list_names]\n",
    "    author_list_urls = [i.encode ('UTF-8') for i in author_list_urls]\n",
    "    url_citations = url_citations.encode ('UTF-8') \n",
    "    ti_text, ab_text, doc_url, = ti_text.encode ('UTF-8'), ab_text.encode ('UTF-8'), doc_url.encode ('UTF-8')\n",
    "\n",
    "    # Build list of all the items to return\n",
    "    # Call to_unicode() defined above to convert strings to unicode\n",
    "    #r = (to_unicode(ti_text), to_unicode(ab_text), doc_url, n_citations, url_citations, \\\n",
    "    #     [to_unicode (i) for i in author_list_names], author_list_urls)\n",
    "    \n",
    "    r = (ti_text, ab_text, doc_url, n_citations, url_citations, \\\n",
    "         author_list_names, author_list_urls)\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to clean up the abstact text\n",
    "\n",
    "1) Remove copyright and funding info.      \n",
    "2) Add space between sentences, if not present.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_copyright_symbol (s):\n",
    "    return s.replace('Â©', 'COPYRIGHT')\n",
    "\n",
    "def is_clean_sentence (s, dirty_list = ['FUNDING','COPYRIGHT','PUBLISH','PUBLISHED']):\n",
    "    s = replace_copyright_symbol (s)\n",
    "    s = s.upper()\n",
    "    for d in dirty_list:\n",
    "        if d in s: return False\n",
    "    return True\n",
    "\n",
    "# Function to separate sentences, where no space is present.  This seems to be issue with\n",
    "# copyright lines and structured abstract headings.  For example, ...Elsevier, Ltd.Chronic...\n",
    "# will add a space after the period.  Likewise, 'of the participants.OBJECTIVES:' will\n",
    "# add space after the period.  Essential, a space will be added after any period that\n",
    "# is directly followed by a letter.\n",
    "\n",
    "def separate_sentences (s):\n",
    "    s = re.sub('\\.[a-zA-z]', lambda x: x.group(0)[0:-1] + ' ' + x.group(0)[-1], s)\n",
    "    return s\n",
    "\n",
    "def clean_abstract (s):\n",
    "    s = replace_copyright_symbol (s) \n",
    "    s = filter (lambda x: x in string.printable, s) # Remove unprintable weirdness.\n",
    "    s = s.strip()\n",
    "    s = separate_sentences (s)\n",
    "    try:\n",
    "        sent_list= sent_detector.tokenize (s)     # Tokenize into sentences\n",
    "        sent_list = filter (is_clean_sentence, sent_list) # Remove ''dirty sentences',\n",
    "                                                      # with funding or copyright info\n",
    "        s = ' '.join (sent_list)                          # Build sentences back to single string\n",
    "    except: print s\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to convert British English to American English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_british_american_map (fp_brit_am):\n",
    "    r = {}\n",
    "    with open (fp_brit_am, 'rU') as f:\n",
    "        myCsvreader = csv.reader (f, delimiter = '\\t',)\n",
    "        for row in myCsvreader:\n",
    "            r[row[0]] = row[1]\n",
    "    return r\n",
    "        \n",
    "def replacer_factory(spelling_dict):\n",
    "    def replacer(match):\n",
    "        word = match.group()\n",
    "        return spelling_dict.get(word, word)\n",
    "    return replacer\n",
    "\n",
    "def limie_to_yankee(text, brit_to_am_dict):\n",
    "    pattern = r'\\b\\w+\\b'  # this pattern matches whole words only\n",
    "    replacer = replacer_factory(brit_to_am_dict)\n",
    "    return re.sub(pattern, replacer, text)\n",
    "\n",
    "#from english_american_dictionary import ame_to_bre_spellings\n",
    "#text = 'I am the conceptualised tyre full of haem'\n",
    "#print limie_to_yankee (text, brit_to_am_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for writing abstracts to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to build simple dictionary from the elements extracted from \n",
    "# a single article's HTML. This dictionary corresponds to JSON that will \n",
    "# be saved in the Mongo db.\n",
    "\n",
    "def build_doc_dict(title='dummyTitle', authors=['I.P. Freely','The other author'], \\\n",
    "                    journal='dummyJournal', year=1776, abstract='dummyAb', \\\n",
    "                    idx_article_in_journal=69, n_cit=69, url_cit=None, url_art=None,\\\n",
    "                    url_authors_list=[], e_issn = 'xxxx-xxxx'):\n",
    "    \n",
    "    return {        'TITLE':title,\\\n",
    "                    'AUTHORS':authors,\\\n",
    "                    'JOURNAL':journal,\n",
    "                    'E_ISSN':e_issn,\\\n",
    "                    'YEAR':year,\\\n",
    "                    'ABSTRACT':abstract, \\\n",
    "                    'ARTICLE_IDX':idx_article_in_journal, \\\n",
    "                    'N_CITATIONS':n_cit,\\\n",
    "                    'URL_CITATIONS':url_cit,\\\n",
    "                    'URL_ARTICLE':url_art,\\\n",
    "                    'URL_AUTHORS_LIST':url_authors_list}\n",
    "\n",
    "##########################################################\n",
    "\n",
    "# Function to open (if extant) or create (if not) the collection from the \n",
    "# Mongo database.  Collection is analogous to a table in SQL db.\n",
    "\n",
    "def create_or_get_collection (db, col_name, verbose = False):\n",
    "    try:\n",
    "        col = db.get_collection (col_name)\n",
    "        if verbose: print 'Returned extant Collection'\n",
    "    except:\n",
    "        col = db.create_collection (col_name)\n",
    "        if verbose: print 'Created Collection'\n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN \n",
    "\n",
    "Load the saved HTML files, use Beautiful Soup to extract the abstracts and other \n",
    "content, and save them to a MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing HTML from file = 2015_00003_1935-8237_01.html\n",
      "Parsing HTML from file = 2015_00004_1529-1006_01.html\n",
      "Parsing HTML from file = 2015_00005_15452085_01.html\n",
      "Parsing HTML from file = 2015_00006_15206890_01.html\n",
      "Parsing HTML from file = 2015_00007_1931-7883_01.html\n",
      "Parsing HTML from file = 2015_00008_14606976_01.html\n",
      "Parsing HTML from file = 2015_00010_1553-877X_01.html\n",
      "Parsing HTML from file = 2013_00000_15390756_01.html\n",
      "Parsing HTML from file = 2013_00002_0079-6425_01.html\n",
      "Parsing HTML from file = 2013_00003_1935-8237_01.html\n",
      "Parsing HTML from file = 2013_00004_1529-1006_01.html\n",
      "Parsing HTML from file = 2013_00007_1931-7883_01.html\n",
      "Parsing HTML from file = 2013_00008_14606976_01.html\n",
      "Parsing HTML from file = 2013_00010_1553-877X_01.html\n",
      "Parsing HTML from file = 2011_00000_15390756_01.html\n",
      "Parsing HTML from file = 2011_00002_0079-6425_01.html\n",
      "Parsing HTML from file = 2011_00003_1935-8237_01.html\n",
      "Parsing HTML from file = 2011_00004_1529-1006_01.html\n",
      "Parsing HTML from file = 2011_00005_15452085_01.html\n",
      "Parsing HTML from file = 2011_00006_15206890_01.html\n",
      "Parsing HTML from file = 2011_00007_1931-7883_01.html\n",
      "Parsing HTML from file = 2011_00008_14606976_01.html\n",
      "Parsing HTML from file = 2011_00010_1553-877X_01.html\n",
      "Parsing HTML from file = 2009_00000_15390756_01.html\n",
      "Parsing HTML from file = 2009_00002_0079-6425_01.html\n",
      "Parsing HTML from file = 2009_00003_1935-8237_01.html\n",
      "Parsing HTML from file = 2009_00004_1529-1006_01.html\n",
      "Parsing HTML from file = 2009_00006_15206890_01.html\n",
      "Parsing HTML from file = 2009_00007_1931-7883_01.html\n",
      "Parsing HTML from file = 2009_00008_14606976_01.html\n",
      "Parsing HTML from file = 2009_00010_1553-877X_01.html\n",
      "Parsing HTML from file = 2007_00000_15390756_01.html\n",
      "Parsing HTML from file = 2007_00004_1529-1006_01.html\n",
      "Parsing HTML from file = 2007_00007_1931-7883_01.html\n",
      "Parsing HTML from file = 2007_00008_14606976_01.html\n",
      "Parsing HTML from file = 2007_00010_1553-877X_01.html\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Directory with the HTML files\n",
    "fp_dir_in = '/Users/bryanfry/projects/proj_asksci/files_out/scopus_query_out/'  \n",
    "\n",
    "#Path to log file with all the scraped HTML pages.  Load it as a pandas dataframe\n",
    "fp_html_log_in = '/Users/bryanfry/projects/proj_asksci/files_out/scopus_query_out/_SCOPUS_QRY_LOG.csv'\n",
    "df_html_all_years = pd.read_csv (fp_html_log_in)\n",
    "df_html_all_years.YEAR = df_html_all_years.YEAR.apply(str) #Convert year to string (not int)\n",
    "\n",
    "# Give path to file mapping British to American spellings, and build dictionary with mappings\n",
    "fp_brit_am = '/Users/bryanfry/projects/proj_asksci/files_in/britsh_to_american.txt'\n",
    "brit_to_am_dict = load_british_american_map (fp_brit_am)\n",
    "\n",
    "# Set the range of HTML files to read (0-indexed)\n",
    "html_idx_start = 0\n",
    "html_idx_end = 10000 # 10000 > number of files --> go to end\n",
    "\n",
    "max_articles_per_html = 200\n",
    "\n",
    "#List of years, as strings\n",
    "year_list = ['2015','2013','2011','2009','2007']\n",
    "\n",
    "db_name = 'abstract_db' #Name of the mongo db\n",
    "col_name = db_name + '_col' #Name of the collection in the db (just need one collection)\n",
    "\n",
    "# Open the database and get / create the collection\n",
    "client = pymongo.MongoClient()\n",
    "db = client.get_database (db_name)\n",
    "col = create_or_get_collection (db, col_name)\n",
    "\n",
    "# Sentence detector -- Punkt tokenizer\n",
    "sent_detector = nltk.data.load ('tokenizers/punkt/english.pickle')\n",
    "\n",
    "for year in year_list:\n",
    "\n",
    "    df_html = df_html_all_years[df_html_all_years.YEAR ==  year]\n",
    "    df_html = df_html.sort_values (['HTML_FILE'])\n",
    "    # Filter chromedriver reset remarks from the dataframe\n",
    "    df_html = df_html[df_html.YEAR.apply (is_chromedriver_remark) == False]\n",
    "\n",
    "    # Now loop on the rows of the log file.  Each one corresponds to a saved HTML file\n",
    "    for html_idx in range (html_idx_start, min ([html_idx_end + 1, len(df_html)])):\n",
    "\n",
    "        html_log_item = df_html.iloc[html_idx]  # Get a row from the HTML log file.\n",
    "        fn_html = html_log_item.HTML_FILE\n",
    "        e_issn = html_log_item.E_ISSN\n",
    "        title_j = html_log_item.JOURNAL_TITLE\n",
    "        SNIP_2015 = html_log_item['2015_SNIP']\n",
    "        n_tot_art = html_log_item.N_ARTICLES\n",
    "        \n",
    "        if not fn_html[0:3] == 'ERR':\n",
    "            \n",
    "            print 'Parsing HTML from file = ' + fn_html\n",
    "\n",
    "            fp_html = os.path.join (fp_dir_in, fn_html) #Prepend dir to get HTML filepath\n",
    "            year, journal_idx, issn, page_idx_in_journal = parse_html_filepath (fp_html) # Get items from HTML filepath\n",
    "\n",
    "            soup, li_list = scopus_query_html_to_soup(fp_html) #Get soup and list of <li> tags\n",
    "\n",
    "            # Now we loop on the individal <li> tags in the soup.  Each one is data for one article\n",
    "            list_doc_dict = []  #Create empty list with the dictionaries containing data for the docs.\n",
    "                                # A new dictionary will be added to the list for each article.\n",
    "            for li_idx, li in enumerate (li_list):\n",
    "\n",
    "                # Compute article index in journal (can be over 200 if there are multiple HTML dumps for \n",
    "                # one journal)\n",
    "                idx_article_in_journal = li_idx + (max_articles_per_html * (page_idx_in_journal))\n",
    "\n",
    "                # Parse HTML for an <li>\n",
    "                (title, abstract, url_art, n_cit, url_citations, auth_list, \\\n",
    "                url_auth_list) = get_doc_info (soup, li_idx, li_list)  \n",
    "\n",
    "                # Convert British to American English, if present\n",
    "                abstract = limie_to_yankee (abstract, brit_to_am_dict)\n",
    "                \n",
    "                # Clean the abstract text (remove copyright line, etc, remove unprintables)\n",
    "                abstract = clean_abstract(abstract)\n",
    "\n",
    "                # Remove non-printable weirdo characters from abstract, authors, Journal title\n",
    "                title_j = remove_unprintable (title_j)\n",
    "                title = remove_unprintable (title)\n",
    "                auth_list = [remove_unprintable (i) for i in auth_list]\n",
    "\n",
    "                # Build the dictionary to write to Mongo (as JSON)\n",
    "                # NOTE: Change the joutnal title to unicode first\n",
    "                doc_dict = build_doc_dict (title, auth_list, to_unicode(title_j), year, abstract, \\\n",
    "                                idx_article_in_journal, n_cit, url_citations, \\\n",
    "                                url_art, url_auth_list, e_issn)\n",
    "\n",
    "                list_doc_dict.append (doc_dict)\n",
    "            col.insert_many (list_doc_dict)\n",
    "\n",
    "print 'DONE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
